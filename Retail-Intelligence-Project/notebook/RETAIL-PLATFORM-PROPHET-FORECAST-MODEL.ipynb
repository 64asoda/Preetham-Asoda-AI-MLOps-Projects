{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd4cbf7-dd2b-40ef-89f0-bf5fc5c2468c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install prophet==1.1.5 \\\n",
    "    convertdate==2.4.0 \\\n",
    "    holidays==0.34 \\\n",
    "    cmdstanpy==1.2.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d8db6c-7610-4cd3-8063-feef023b47c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORT LIBRATIES\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import pickle\n",
    "from prophet import Prophet\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42b7fca-55ba-46c8-b554-8b48264d337f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. CONFIGURATION AND UTILS\n",
    "# ==============================================================================\n",
    "\n",
    "INPUT_TABLE_NAME = \"retail_feature_engg_done\"\n",
    "FORECAST_DAYS = 30\n",
    "FORECAST_TYPE = 'daily_revenue' # options: 'daily_revenue', 'daily_orders'\n",
    "TARGET_METRIC = 'mape' # Metric to log/optimize\n",
    "\n",
    "# DATABASE CONNECTION\n",
    "pg_host = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"host\")\n",
    "pg_port = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"port\")\n",
    "pg_database = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"database\")\n",
    "pg_username = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"username\")\n",
    "pg_password = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"password\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{pg_host}:{pg_port}/{pg_database}\"\n",
    "CONNECTION_PROPERTIES = {\n",
    "    \"user\": pg_username,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45746aca-0151-4413-98ef-cc4884f0d1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. READ AND CONVERT TO PANDAS\n",
    "# ==============================================================================\n",
    "\n",
    "def read_and_prepare_data(table_name: str, forecast_type: str, spark: SparkSession) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data from PostgreSQL using PySpark, converts to Pandas, and prepares \n",
    "    the DataFrame for Prophet (aggregation, cleaning, feature engineering).\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from PostgreSQL table: {table_name}...\")\n",
    "    \n",
    "    # 1. READ DATA via PySpark\n",
    "    spark_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=CONNECTION_PROPERTIES)\n",
    "    df = spark_df.toPandas()\n",
    "    \n",
    "    \n",
    "    # Convert purchase_datetime to datetime\n",
    "    df['purchase_datetime'] = pd.to_datetime(df['purchase_datetime'])\n",
    "\n",
    "    print(f\"Loaded {len(df)} records...\")\n",
    "    \n",
    "    # Filter out sparse early data\n",
    "    start_date = pd.to_datetime('2017-09-01')\n",
    "    df = df[df['purchase_datetime'] >= start_date].copy()\n",
    "    \n",
    "    print(f\"âœ“ Filtered to start from: {start_date}\")\n",
    "\n",
    "    # 2. Aggregate based on forecast type\n",
    "    if forecast_type == 'daily_revenue':\n",
    "        df = df.groupby(df['purchase_datetime'].dt.date).agg({'total_payment_value':'sum'}).reset_index()\n",
    "        df.columns = ['ds', 'y']\n",
    "        print(f\"Aggregated to daily revenue: {len(df)} days\")\n",
    "\n",
    "    elif forecast_type == 'daily_orders':\n",
    "        df = df.groupby(df['purchase_datetime'].dt.date).agg({'order_id':'nunique'}).reset_index()\n",
    "        df.columns = ['ds', 'y']\n",
    "        print(f\"Aggregated to daily orders: {len(df)} days\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown forecast_type: {forecast_type}\")\n",
    "\n",
    "    # Ensure column 'ds' is datetime\n",
    "    df['ds'] = pd.to_datetime(df['ds'])\n",
    "\n",
    "    # 3. Clean and Transform\n",
    "    # Remove extreme outliers using IQR\n",
    "    q1, q3 = df['y'].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    df['y'] = df['y'].clip(lower, upper)\n",
    "    \n",
    "    # Log-transform target to stabilize variance\n",
    "    df['y_raw'] = df['y'] # Keeping raw value for inverse transformation\n",
    "    df['y'] = np.log1p(df['y'])\n",
    "    \n",
    "    # 4. Add Regressors\n",
    "    df['weekday'] = df['ds'].dt.weekday\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['is_weekend'] = df['ds'].dt.weekday >= 5\n",
    "    \n",
    "    print(f\"âœ“ Target variable range: {df['y'].min():.2f} to {df['y'].max():.2f}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "615c4d63-1341-41ee-843d-cdb9ea18d7b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. BUILD AND FIT PROPHET MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "def train_prophet_model(df):\n",
    "    \"\"\"\n",
    "    Create and fit Prophet model and return the model object.\n",
    "    \"\"\"\n",
    "    model = Prophet(\n",
    "        seasonality_mode = 'multiplicative',\n",
    "        changepoint_prior_scale = 0.05,\n",
    "        seasonality_prior_scale = 10,\n",
    "        yearly_seasonality = True,\n",
    "        weekly_seasonality = True,\n",
    "        daily_seasonality = False)\n",
    "    \n",
    "    # Add Brazilian holidays\n",
    "    model.add_country_holidays(country_name='BR')\n",
    "    print(\"Added Brazilian Holidays\")\n",
    "\n",
    "    # Add regressors\n",
    "    model.add_regressor('weekday')\n",
    "    model.add_regressor('month')\n",
    "    model.add_regressor('is_weekend')\n",
    "\n",
    "    # Fit Model\n",
    "    print(\"Training Prophet Model\")\n",
    "    model.fit(df)\n",
    "    print(\"Model trained successfully\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e66bdc2-de8c-4d89-8a0d-541933def2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. GENERATE AND INVERSE-TRANSFORM FORECASTS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_and_postprocess_forecast(model, df: pd.DataFrame, periods: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate future predictions and apply inverse log-transform.\n",
    "    \"\"\"\n",
    "    # 1. Create future dataframe and add regressors\n",
    "    future = model.make_future_dataframe(periods=periods, include_history=False) # Only future periods\n",
    "    \n",
    "    # Re-apply regressors to the future dataframe\n",
    "    future['weekday'] = future['ds'].dt.weekday\n",
    "    future['month'] = future['ds'].dt.month\n",
    "    future['is_weekend'] = future['ds'].dt.weekday >= 5\n",
    "    \n",
    "    # 2. Predict\n",
    "    forecast_future = model.predict(future)\n",
    "\n",
    "    # 3. Generate predictions for the historical period (for evaluation)\n",
    "    forecast_history = model.predict(df[['ds', 'weekday', 'month', 'is_weekend']])\n",
    "    \n",
    "    # 4. Combine and Inverse Log-Transform\n",
    "    # Select only the relevant columns from history and future\n",
    "    cols_to_keep = ['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'trend']\n",
    "    forecast = pd.concat([\n",
    "        forecast_history[cols_to_keep], \n",
    "        forecast_future[cols_to_keep]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Inverse log-transform for all columns\n",
    "    for col in ['yhat', 'yhat_lower', 'yhat_upper']:\n",
    "        forecast[col] = np.expm1(forecast[col])\n",
    "\n",
    "    print(f\"Generated {len(forecast)} total days of forecasts (history + {periods} future)\")\n",
    "\n",
    "    return forecast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4febe2-1b5f-4fe7-820a-1be69c084e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. EVALUATE MODEL AND LOG METRICS\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_and_log_metrics(df_original: pd.DataFrame, forecast: pd.DataFrame, target_metric: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate Prophet forecast vs historical actuals and log metrics.\n",
    "    Handles Databricks/Spark nested types, list-wrapped values, dicts, and dtype issues.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------\n",
    "    # 1. Select matching historical data (exclude future forecast)\n",
    "    # ------------------------------------------------------------\n",
    "    cutoff_date = forecast['ds'].max() - pd.Timedelta(days=FORECAST_DAYS)\n",
    "    df_history = df_original[df_original['ds'] <= cutoff_date].copy()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. Merge historical with forecasted yhat\n",
    "    # ------------------------------------------------------------\n",
    "    comparison = df_history.merge(\n",
    "        forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']],\n",
    "        on='ds', \n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Use the raw non-log-transformed target\n",
    "    if 'y_raw' in comparison.columns:\n",
    "        comparison['y'] = comparison['y_raw']\n",
    "        comparison.drop(columns=['y_raw'], inplace=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. Flatten function for nested lists/arrays/dicts\n",
    "    # ------------------------------------------------------------\n",
    "    def flatten_to_scalar(x):\n",
    "        if isinstance(x, (list, tuple, np.ndarray)):\n",
    "            if len(x) == 0:\n",
    "                return np.nan\n",
    "            return flatten_to_scalar(x[0])\n",
    "        if isinstance(x, dict):\n",
    "            if len(x) == 0:\n",
    "                return np.nan\n",
    "            return flatten_to_scalar(list(x.values())[0])\n",
    "        if hasattr(x, \"asDict\"):  # Spark Row\n",
    "            vals = list(x.asDict().values())\n",
    "            return flatten_to_scalar(vals[0]) if vals else np.nan\n",
    "        # If it's already a scalar, return it\n",
    "        return x\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 4. Convert to numeric safely - ELEMENT BY ELEMENT\n",
    "    # ------------------------------------------------------------\n",
    "    # First flatten, then convert\n",
    "    comparison['y'] = comparison['y'].apply(flatten_to_scalar)\n",
    "    comparison['yhat'] = comparison['yhat'].apply(flatten_to_scalar)\n",
    "    \n",
    "    # Convert using a safer approach that maintains index alignment\n",
    "    def safe_convert_to_float(val):\n",
    "        try:\n",
    "            return float(val)\n",
    "        except (TypeError, ValueError):\n",
    "            return np.nan\n",
    "    \n",
    "    comparison['y'] = comparison['y'].apply(safe_convert_to_float)\n",
    "    comparison['yhat'] = comparison['yhat'].apply(safe_convert_to_float)\n",
    "\n",
    "    # Drop rows where conversion failed\n",
    "    initial_len = len(comparison)\n",
    "    comparison.dropna(subset=['y', 'yhat'], inplace=True)\n",
    "    final_len = len(comparison)\n",
    "    \n",
    "    if initial_len != final_len:\n",
    "        print(f\"Dropped {initial_len - final_len} rows due to NaN values\")\n",
    "    \n",
    "    # Ensure we have data to evaluate\n",
    "    if len(comparison) == 0:\n",
    "        raise ValueError(\"No valid data points after cleaning. Check data types in your DataFrame.\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 5. Apply floor to avoid divide-by-zero in MAPE/SMAPE\n",
    "    # ------------------------------------------------------------\n",
    "    comparison['y'] = comparison['y'].clip(lower=100)\n",
    "    comparison['yhat'] = comparison['yhat'].clip(lower=100)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 6. Compute error metrics\n",
    "    # ------------------------------------------------------------\n",
    "    comparison['error'] = comparison['y'] - comparison['yhat']\n",
    "    comparison['abs_error'] = comparison['error'].abs()\n",
    "\n",
    "    mae  = comparison['abs_error'].mean()\n",
    "    mape = (comparison['abs_error'] / comparison['y']).mean() * 100\n",
    "    rmse = np.sqrt((comparison['error'] ** 2).mean())\n",
    "    smape = (\n",
    "        100 / len(comparison) *\n",
    "        np.sum(\n",
    "            2 * np.abs(comparison['y'] - comparison['yhat']) /\n",
    "            (np.abs(comparison['y']) + np.abs(comparison['yhat']))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 7. Log metrics to MLflow\n",
    "    # ------------------------------------------------------------\n",
    "    metrics = {'MAE': mae, 'MAPE': mape, 'RMSE': rmse, 'SMAPE': smape}\n",
    "    mlflow.log_metrics({f\"eval_{k}\": float(v) for k, v in metrics.items()})\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # 8. Pretty print metrics\n",
    "    # ------------------------------------------------------------\n",
    "    print(\"=\"*60)\n",
    "    print(\"MODEL PERFORMANCE METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"MAE  (Mean Absolute Error):          ${mae:,.2f}\")\n",
    "    print(f\"MAPE (Mean Absolute % Error):        {mape:.2f}%\")\n",
    "    print(f\"RMSE (Root Mean Squared Error):      ${rmse:,.2f}\")\n",
    "    print(f\"SMAPE (Symmetric Mean Abs % Error):  {smape:.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return metrics[target_metric.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8332e743-3f56-4971-87cf-21ef86eecc71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. EXECUTION BLOCK (MLFLOW INTEGRATION)\n",
    "# ==============================================================================\n",
    "\n",
    "def main_prophet_run(input_table, forecast_type, forecast_days, target_metric):\n",
    "    \"\"\"\n",
    "    Main function to run the Prophet model pipeline and log results to MLflow.\n",
    "    \"\"\"\n",
    "    # Use a specific run name\n",
    "    run_name = f\"Prophet_Forecast_{forecast_type}\"\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        \n",
    "        print(f\"--- Starting Prophet Pipeline for {forecast_type} ---\")\n",
    "        \n",
    "        # 1. Load and Prepare Data\n",
    "        df = read_and_prepare_data(input_table, forecast_type, spark)\n",
    "        \n",
    "        # 2. Log Model Parameters to MLflow\n",
    "        mlflow.log_params({\n",
    "            \"forecast_type\": forecast_type,\n",
    "            \"forecast_days\": forecast_days,\n",
    "            \"seasonality_mode\": 'multiplicative',\n",
    "            \"changepoint_prior_scale\": 0.05,\n",
    "            \"seasonality_prior_scale\": 10,\n",
    "            \"data_start_date\": df['ds'].min().strftime('%Y-%m-%d'),\n",
    "            \"data_end_date\": df['ds'].max().strftime('%Y-%m-%d')\n",
    "        })\n",
    "        \n",
    "        # 3. Train Model\n",
    "        model = train_prophet_model(df)\n",
    "        \n",
    "        # 4. Generate Forecasts (including historical predictions for evaluation)\n",
    "        forecast = generate_and_postprocess_forecast(model, df, periods=forecast_days)\n",
    "        \n",
    "        # 5. Evaluate and Log Metrics\n",
    "        metric_value = evaluate_and_log_metrics(df, forecast, target_metric)\n",
    "        \n",
    "        # 6. Save Artifacts\n",
    "        \n",
    "        # a. Save Prophet Model\n",
    "        model_path = \"prophet_model.pkl\"\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        mlflow.log_artifact(model_path, \"prophet_artifacts\")\n",
    "        \n",
    "        # b. Save Forecast CSV (for reporting)\n",
    "        output_df = forecast.tail(forecast_days).copy()\n",
    "        output_df = output_df[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'trend']]\n",
    "        output_df.columns = ['date', 'predicted_value', 'lower_bound', 'upper_bound', 'trend']\n",
    "        output_df['model_run_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        filename_csv = f'prophet_forecast_future.csv'\n",
    "        output_df.to_csv(filename_csv, index=False)\n",
    "        mlflow.log_artifact(filename_csv, \"forecast_output\")\n",
    "        \n",
    "        print(f\"\\nModel and results logged to MLflow run: {run.info.run_uuid}\")\n",
    "\n",
    "        # 7. Visualization\n",
    "        fig = model.plot(forecast, figsize=(12, 6))\n",
    "        plt.title(f'Prophet Forecast for {forecast_type.replace(\"_\", \" \").title()}', fontsize=14)\n",
    "        \n",
    "        # Log the figure to MLflow\n",
    "        fig_path = f\"forecast_plot_{forecast_type}.png\"\n",
    "        fig.savefig(fig_path)\n",
    "        mlflow.log_artifact(fig_path, \"visualizations\")\n",
    "        plt.close(fig) # Closing plot to prevent memory leaks\n",
    "\n",
    "        print(f\"\\nProphet Pipeline Completed Successfully!\")\n",
    "        \n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca0e610-4d3b-4f15-8700-9dc93bcb7511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Prophet Pipeline for daily_revenue ---\n",
      "Reading data from PostgreSQL table: retail_feature_engg_done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:23:02 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 104023 records...\n",
      "âœ“ Filtered to start from: 2017-09-01 00:00:00\n",
      "Aggregated to daily revenue: 364 days\n",
      "âœ“ Target variable range: 7.55 to 11.10\n",
      "Added Brazilian Holidays\n",
      "Training Prophet Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:23:02 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully\n",
      "Generated 394 total days of forecasts (history + 30 future)\n",
      "============================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "============================================================\n",
      "MAE  (Mean Absolute Error):          $5,065.48\n",
      "MAPE (Mean Absolute % Error):        16.54%\n",
      "RMSE (Root Mean Squared Error):      $6,697.59\n",
      "SMAPE (Symmetric Mean Abs % Error):  15.78%\n",
      "============================================================\n",
      "\n",
      "Model and results logged to MLflow run: bac20731bceb42229f9b064ac47557fd\n",
      "\n",
      "âœ¨ Prophet Pipeline Completed Successfully!\n",
      "ðŸƒ View run Prophet_Forecast_daily_revenue at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/2912528436743962/runs/bac20731bceb42229f9b064ac47557fd\n",
      "ðŸ§ª View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/2912528436743962\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ds</th><th>yhat</th><th>yhat_lower</th><th>yhat_upper</th><th>trend</th></tr></thead><tbody><tr><td>2018-09-04T00:00:00Z</td><td>3080.4098815001207</td><td>2388.6503141395406</td><td>4017.48509934458</td><td>9.015676393425144</td></tr><tr><td>2018-09-05T00:00:00Z</td><td>2653.170236476089</td><td>2008.6230760768965</td><td>3510.449588074229</td><td>9.0078954448152</td></tr><tr><td>2018-09-06T00:00:00Z</td><td>2492.3876872312167</td><td>1932.186242268804</td><td>3249.139347769086</td><td>9.000114496205256</td></tr><tr><td>2018-09-07T00:00:00Z</td><td>1656.537559803055</td><td>1274.588201262273</td><td>2201.392827919716</td><td>8.992333547595313</td></tr><tr><td>2018-09-08T00:00:00Z</td><td>1675.4351142173016</td><td>1264.0777627523203</td><td>2152.5336815388187</td><td>8.984552598985369</td></tr><tr><td>2018-09-09T00:00:00Z</td><td>1714.8331520016432</td><td>1305.8546457892646</td><td>2267.844571471202</td><td>8.976771650375424</td></tr><tr><td>2018-09-10T00:00:00Z</td><td>2158.085605427028</td><td>1641.7752118005774</td><td>2843.0570950553156</td><td>8.968990701765481</td></tr><tr><td>2018-09-11T00:00:00Z</td><td>2085.664905481449</td><td>1588.1965992566672</td><td>2706.8208552519523</td><td>8.961209753155536</td></tr><tr><td>2018-09-12T00:00:00Z</td><td>1921.6158319030737</td><td>1468.7012239815888</td><td>2521.5030116484204</td><td>8.953428804545593</td></tr><tr><td>2018-09-13T00:00:00Z</td><td>1925.5508100555749</td><td>1495.179780814303</td><td>2530.926943103864</td><td>8.94564785593565</td></tr><tr><td>2018-09-14T00:00:00Z</td><td>1754.804299471987</td><td>1340.7530125768646</td><td>2284.30653577578</td><td>8.937866907325706</td></tr><tr><td>2018-09-15T00:00:00Z</td><td>1460.2290094136238</td><td>1121.3051876208156</td><td>1885.8014508065946</td><td>8.930085958715763</td></tr><tr><td>2018-09-16T00:00:00Z</td><td>1575.0768835579572</td><td>1198.3828756116475</td><td>2064.1383743144315</td><td>8.922305010105818</td></tr><tr><td>2018-09-17T00:00:00Z</td><td>2075.815977664769</td><td>1552.3885190252215</td><td>2697.908844482221</td><td>8.914524061495875</td></tr><tr><td>2018-09-18T00:00:00Z</td><td>2092.7499429746235</td><td>1580.462906093482</td><td>2740.26641039823</td><td>8.90674311288593</td></tr><tr><td>2018-09-19T00:00:00Z</td><td>2000.40127635097</td><td>1593.0402419003717</td><td>2696.594521328922</td><td>8.898962164275988</td></tr><tr><td>2018-09-20T00:00:00Z</td><td>2066.2865727639078</td><td>1569.900458736929</td><td>2673.778864935118</td><td>8.891181215666043</td></tr><tr><td>2018-09-21T00:00:00Z</td><td>1930.7086002566887</td><td>1454.2983513674073</td><td>2583.3413455812915</td><td>8.8834002670561</td></tr><tr><td>2018-09-22T00:00:00Z</td><td>1638.543846295598</td><td>1258.0520063115384</td><td>2129.122142286893</td><td>8.875619318446155</td></tr><tr><td>2018-09-23T00:00:00Z</td><td>1789.4978047968968</td><td>1362.8002878660657</td><td>2331.5117082444003</td><td>8.867838369836212</td></tr><tr><td>2018-09-24T00:00:00Z</td><td>2372.1457377317815</td><td>1817.812809124447</td><td>3050.481482536765</td><td>8.860057421226267</td></tr><tr><td>2018-09-25T00:00:00Z</td><td>2397.4572502646115</td><td>1834.6767586607295</td><td>3177.4178580866987</td><td>8.852276472616325</td></tr><tr><td>2018-09-26T00:00:00Z</td><td>2287.8082370804163</td><td>1753.4500609315921</td><td>3009.850685389436</td><td>8.844495524006382</td></tr><tr><td>2018-09-27T00:00:00Z</td><td>2348.7219751927596</td><td>1769.6123462634969</td><td>3073.1869521135145</td><td>8.836714575396439</td></tr><tr><td>2018-09-28T00:00:00Z</td><td>2175.1891475681614</td><td>1688.8756586654758</td><td>2874.632569092254</td><td>8.828933626786492</td></tr><tr><td>2018-09-29T00:00:00Z</td><td>1825.7247548492712</td><td>1380.943808593734</td><td>2439.7930302928676</td><td>8.821152678176551</td></tr><tr><td>2018-09-30T00:00:00Z</td><td>1964.714814006103</td><td>1493.5253338974378</td><td>2543.014211261758</td><td>8.813371729566605</td></tr><tr><td>2018-10-01T00:00:00Z</td><td>2465.856865446255</td><td>1854.4762379890255</td><td>3217.745282257542</td><td>8.805590780956663</td></tr><tr><td>2018-10-02T00:00:00Z</td><td>2451.044349638622</td><td>1881.1192413473054</td><td>3277.4221252065086</td><td>8.797809832346719</td></tr><tr><td>2018-10-03T00:00:00Z</td><td>2300.4561901967563</td><td>1731.400517747957</td><td>3005.241853613596</td><td>8.790028883736776</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2018-09-04T00:00:00Z",
         3080.4098815001207,
         2388.6503141395406,
         4017.48509934458,
         9.015676393425144
        ],
        [
         "2018-09-05T00:00:00Z",
         2653.170236476089,
         2008.6230760768965,
         3510.449588074229,
         9.0078954448152
        ],
        [
         "2018-09-06T00:00:00Z",
         2492.3876872312167,
         1932.186242268804,
         3249.139347769086,
         9.000114496205256
        ],
        [
         "2018-09-07T00:00:00Z",
         1656.537559803055,
         1274.588201262273,
         2201.392827919716,
         8.992333547595313
        ],
        [
         "2018-09-08T00:00:00Z",
         1675.4351142173016,
         1264.0777627523203,
         2152.5336815388187,
         8.984552598985369
        ],
        [
         "2018-09-09T00:00:00Z",
         1714.8331520016432,
         1305.8546457892646,
         2267.844571471202,
         8.976771650375424
        ],
        [
         "2018-09-10T00:00:00Z",
         2158.085605427028,
         1641.7752118005774,
         2843.0570950553156,
         8.968990701765481
        ],
        [
         "2018-09-11T00:00:00Z",
         2085.664905481449,
         1588.1965992566672,
         2706.8208552519523,
         8.961209753155536
        ],
        [
         "2018-09-12T00:00:00Z",
         1921.6158319030737,
         1468.7012239815888,
         2521.5030116484204,
         8.953428804545593
        ],
        [
         "2018-09-13T00:00:00Z",
         1925.5508100555749,
         1495.179780814303,
         2530.926943103864,
         8.94564785593565
        ],
        [
         "2018-09-14T00:00:00Z",
         1754.804299471987,
         1340.7530125768646,
         2284.30653577578,
         8.937866907325706
        ],
        [
         "2018-09-15T00:00:00Z",
         1460.2290094136238,
         1121.3051876208156,
         1885.8014508065946,
         8.930085958715763
        ],
        [
         "2018-09-16T00:00:00Z",
         1575.0768835579572,
         1198.3828756116475,
         2064.1383743144315,
         8.922305010105818
        ],
        [
         "2018-09-17T00:00:00Z",
         2075.815977664769,
         1552.3885190252215,
         2697.908844482221,
         8.914524061495875
        ],
        [
         "2018-09-18T00:00:00Z",
         2092.7499429746235,
         1580.462906093482,
         2740.26641039823,
         8.90674311288593
        ],
        [
         "2018-09-19T00:00:00Z",
         2000.40127635097,
         1593.0402419003717,
         2696.594521328922,
         8.898962164275988
        ],
        [
         "2018-09-20T00:00:00Z",
         2066.2865727639078,
         1569.900458736929,
         2673.778864935118,
         8.891181215666043
        ],
        [
         "2018-09-21T00:00:00Z",
         1930.7086002566887,
         1454.2983513674073,
         2583.3413455812915,
         8.8834002670561
        ],
        [
         "2018-09-22T00:00:00Z",
         1638.543846295598,
         1258.0520063115384,
         2129.122142286893,
         8.875619318446155
        ],
        [
         "2018-09-23T00:00:00Z",
         1789.4978047968968,
         1362.8002878660657,
         2331.5117082444003,
         8.867838369836212
        ],
        [
         "2018-09-24T00:00:00Z",
         2372.1457377317815,
         1817.812809124447,
         3050.481482536765,
         8.860057421226267
        ],
        [
         "2018-09-25T00:00:00Z",
         2397.4572502646115,
         1834.6767586607295,
         3177.4178580866987,
         8.852276472616325
        ],
        [
         "2018-09-26T00:00:00Z",
         2287.8082370804163,
         1753.4500609315921,
         3009.850685389436,
         8.844495524006382
        ],
        [
         "2018-09-27T00:00:00Z",
         2348.7219751927596,
         1769.6123462634969,
         3073.1869521135145,
         8.836714575396439
        ],
        [
         "2018-09-28T00:00:00Z",
         2175.1891475681614,
         1688.8756586654758,
         2874.632569092254,
         8.828933626786492
        ],
        [
         "2018-09-29T00:00:00Z",
         1825.7247548492712,
         1380.943808593734,
         2439.7930302928676,
         8.821152678176551
        ],
        [
         "2018-09-30T00:00:00Z",
         1964.714814006103,
         1493.5253338974378,
         2543.014211261758,
         8.813371729566605
        ],
        [
         "2018-10-01T00:00:00Z",
         2465.856865446255,
         1854.4762379890255,
         3217.745282257542,
         8.805590780956663
        ],
        [
         "2018-10-02T00:00:00Z",
         2451.044349638622,
         1881.1192413473054,
         3277.4221252065086,
         8.797809832346719
        ],
        [
         "2018-10-03T00:00:00Z",
         2300.4561901967563,
         1731.400517747957,
         3005.241853613596,
         8.790028883736776
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ds",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "yhat",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "yhat_lower",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "yhat_upper",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "trend",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7. EXECUTION OF THE MAIN RUN\n",
    "# ==============================================================================\n",
    "\n",
    "from prophet import Prophet \n",
    "\n",
    "final_forecast = main_prophet_run(INPUT_TABLE_NAME, FORECAST_TYPE, FORECAST_DAYS, TARGET_METRIC)\n",
    "\n",
    "# Snippet of the final forecast\n",
    "display(final_forecast.tail(FORECAST_DAYS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cb027c-44bc-4837-81b3-eee1b2cc0a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "102f0e9c-3c2e-4e9a-9941-d7ea66009b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f132f367-9c15-4e42-b1b3-d3493099e793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0837a0da-edd7-4110-af01-ee2ad34561fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RETAIL-PLATFORM-PROPHET-FORECAST-MODEL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
