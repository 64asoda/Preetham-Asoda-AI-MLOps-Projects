{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7e8d17-353d-414a-a1b0-cf8ea0c343b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. IMPORT LIBRATIES\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature, Schema, ColSpec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a530e065-b9a3-4498-a9ef-40bec56a9306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. CONFIGURATION AND UTILS\n",
    "# ==============================================================================\n",
    "\n",
    "pg_host = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"host\")\n",
    "pg_port = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"port\")\n",
    "pg_database = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"database\")\n",
    "pg_username = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"username\")\n",
    "pg_password = dbutils.secrets.get(scope=\"postgres-secrets\", key=\"password\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{pg_host}:{pg_port}/{pg_database}\"\n",
    "CONNECTION_PROPERTIES = {\n",
    "    \"user\": pg_username,\n",
    "    \"password\": pg_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# --- Target Table Name ---\n",
    "INPUT_TABLE_NAME = \"retail_feature_engg_done\"\n",
    "OUTPUT_TABLE_NAME = \"two_tower_trained_model_metadata\"\n",
    "\n",
    "#Embedding dimensions\n",
    "embedding_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "028be17e-d2e9-4d87-b164-17dc7f2dbb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. READ AND CONVERT TO PANDAS\n",
    "# ==============================================================================\n",
    "\n",
    "def read_and_convert_data(table_name: str, spark: SparkSession) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data from PostgreSQL using PySpark and converts it to a \n",
    "    single Pandas DataFrame for in-memory processing/training.\n",
    "    \"\"\"\n",
    "    print(f\"Reading data from PostgreSQL table: {table_name}...\")\n",
    "    \n",
    "    # 1. Read using Spark\n",
    "    spark_df = spark.read.jdbc(url=jdbc_url, table=table_name, properties=CONNECTION_PROPERTIES)\n",
    "    \n",
    "    # 2. Convert to Pandas - NECESSARY STEP FOR YOUR PYTORCH/PANDAS CODE\n",
    "    # WARNING: This step can fail if the data is too large for a single node's memory.\n",
    "    df_pandas = spark_df.toPandas()\n",
    "    \n",
    "    print(f\"Read and converted {len(df_pandas):,} records to Pandas.\")\n",
    "    return df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bcf821-c9a2-4432-a97a-4f311a4ca26f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. MODEl/FEATURE CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# User Numerical Features\n",
    "USER_NUM_BASE = [\n",
    "    'total_payment_value_mean', 'total_payment_value_sum',\n",
    "    'item_price_mean', 'item_price_median', 'item_price_max', 'item_price_sum',\n",
    "    'days_since_last_purchase', \n",
    "    'order_id_nunique', 'product_category_nunique', 'product_id_nunique',\n",
    "    'product_weight_kg_mean'\n",
    "]\n",
    "\n",
    "# Items Numerical Features\n",
    "ITEMS_NUM_BASE = [\n",
    "    'item_price_mean', 'item_price_median', 'item_price_max', 'item_price_min', \n",
    "    'item_price_std', 'item_price_sum', \n",
    "    'product_weight_kg_mean',\n",
    "    'unique_orders', 'unique_customers', 'order_count',\n",
    "    'days_since_last_sale']\n",
    "\n",
    "# User Categorical Features\n",
    "USER_CAT_BASE = [\n",
    "    'customer_key_int' \n",
    "]\n",
    "\n",
    "# Items Categorical Features\n",
    "ITEMS_CAT_BASE = [\n",
    "    'product_id_int', \n",
    "    'product_category_mode_int', \n",
    "    'seller_id_mode_int'\n",
    "]\n",
    "\n",
    "\n",
    "def prepare_two_tower_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Performs all Pandas feature engineering, scaling, and prepares PyTorch DataLoaders.\n",
    "    \"\"\"\n",
    "    # 1. Initial Cleaning and Date Conversion\n",
    "    df['purchase_datetime'] = pd.to_datetime(df['purchase_datetime'])\n",
    "\n",
    "    # 2. User Features Aggregation and Flattening\n",
    "    user_features = df.groupby(\"customer_key\").agg({\n",
    "        \"total_payment_value\":[\"mean\",\"sum\"],\n",
    "        \"review_score\":\"mean\",\n",
    "        \"order_id\":\"nunique\",\n",
    "        \"product_category\":\"nunique\",\n",
    "        \"product_id\":\"nunique\",\n",
    "        \"product_weight_kg\":\"mean\",\n",
    "        \"purchase_time_segment_late_night\":\"mean\",\n",
    "        \"purchase_time_segment_morning\":\"mean\",\n",
    "        \"purchase_time_segment_afternoon\":\"mean\",\n",
    "        \"item_price\":[\"mean\", \"median\", \"max\", \"sum\"],\n",
    "        \"purchase_time_segment_evening\":\"mean\"}).reset_index()\n",
    "    \n",
    "    user_features.columns = [f\"{col[0]}_{col[1]}\" if isinstance(col,tuple) and col[1] else col[0] for col in user_features.columns]\n",
    "    \n",
    "    # 3. User Recency\n",
    "    reference_date_user_feat_recency = df['purchase_datetime'].max()\n",
    "    last_purchase = df.groupby(\"customer_key\")[\"purchase_datetime\"].max().reset_index().rename(columns={\"purchase_datetime\":\"last_purchase_datetime\"})\n",
    "    last_purchase[\"days_since_last_purchase\"] = ((reference_date_user_feat_recency - last_purchase[\"last_purchase_datetime\"]).dt.days)\n",
    "    user_features = user_features.merge(last_purchase[[\"customer_key\",\"days_since_last_purchase\"]], on=\"customer_key\", how=\"left\")\n",
    "    \n",
    "    # 4. Item Features Aggregation\n",
    "    items_features = df.groupby(\"product_id\").agg(\n",
    "        product_category_mode = (\"product_category\", lambda x: x.mode()[0]),\n",
    "        seller_id_mode = (\"seller_id\", lambda x: x.mode()[0]),\n",
    "        item_price_mean = (\"item_price\", \"mean\"),\n",
    "        item_price_median = (\"item_price\", \"median\"),\n",
    "        item_price_max = (\"item_price\", \"max\"),\n",
    "        item_price_min = (\"item_price\", \"min\"),\n",
    "        item_price_std = (\"item_price\", \"std\"),\n",
    "        item_price_sum = (\"item_price\", \"sum\"),\n",
    "        product_weight_kg_mean = (\"product_weight_kg\", \"mean\"),\n",
    "        review_score_mean = (\"review_score\", \"mean\"),\n",
    "        unique_orders = (\"order_id\", \"nunique\"),\n",
    "        unique_customers = (\"customer_key\", \"nunique\"),\n",
    "        order_count = (\"order_id\", \"size\"),\n",
    "        last_purchase = (\"purchase_datetime\", \"max\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # 5. Item Recency\n",
    "    reference_date_items_feat_recency = df['purchase_datetime'].max()\n",
    "    last_sale = df.groupby(\"product_id\")[\"purchase_datetime\"].max().reset_index().rename(columns={\"purchase_datetime\":\"last_sale_datetime\"})\n",
    "    last_sale[\"days_since_last_sale\"] = ((reference_date_items_feat_recency - last_sale[\"last_sale_datetime\"]).dt.days)\n",
    "    items_features = items_features.merge(last_sale[[\"product_id\",\"days_since_last_sale\"]], on=\"product_id\", how=\"left\")\n",
    "    \n",
    "    # 6. Categorical Indexing - DO THIS BEFORE THE TRAIN/TEST SPLIT\n",
    "    user_features[\"customer_key_int\"] = user_features[\"customer_key\"].astype('category').cat.codes\n",
    "    items_features[\"product_id_int\"] = items_features[\"product_id\"].astype('category').cat.codes\n",
    "    items_features[\"product_category_mode_int\"] = items_features[\"product_category_mode\"].astype('category').cat.codes\n",
    "    items_features[\"seller_id_mode_int\"] = items_features[\"seller_id_mode\"].astype('category').cat.codes\n",
    "    \n",
    "    # 7. Scaling\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    user_features[USER_NUM_BASE] = scaler.fit_transform(user_features[USER_NUM_BASE])\n",
    "    \n",
    "    scaler_items = StandardScaler()\n",
    "    items_features[ITEMS_NUM_BASE] = scaler_items.fit_transform(items_features[ITEMS_NUM_BASE]) \n",
    "    \n",
    "    # 8. Final Merge \n",
    "    user_cat_cols = USER_CAT_BASE\n",
    "    items_cat_cols = ITEMS_CAT_BASE\n",
    "\n",
    "    user_cols_to_merge_base = ['customer_key'] + user_cat_cols + USER_NUM_BASE\n",
    "    items_cols_to_merge_base = ['product_id'] + items_cat_cols + ITEMS_NUM_BASE \n",
    "\n",
    "    user_features_to_add = user_features[user_cols_to_merge_base].copy()\n",
    "    user_num_rename_map = {col: f\"{col}_user\" for col in USER_NUM_BASE}\n",
    "    user_features_to_add.rename(columns=user_num_rename_map, inplace=True)\n",
    "\n",
    "    items_features_to_add = items_features[items_cols_to_merge_base].copy()\n",
    "    item_num_rename_map = {col: f\"{col}_items\" for col in ITEMS_NUM_BASE} \n",
    "    items_features_to_add.rename(columns=item_num_rename_map, inplace=True)\n",
    "\n",
    "    interaction_data = df[['customer_key', 'product_id']].copy()\n",
    "    interaction_data.drop_duplicates(inplace=True)\n",
    "\n",
    "    interaction_data = interaction_data.merge(\n",
    "        user_features_to_add, \n",
    "        on=\"customer_key\", \n",
    "        how=\"left\"\n",
    "    ) \n",
    "\n",
    "    interaction_data = interaction_data.merge(\n",
    "        items_features_to_add, \n",
    "        on=\"product_id\", \n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    user_num_cols_final = list(user_num_rename_map.values())\n",
    "    item_num_cols_final = list(item_num_rename_map.values())\n",
    "    \n",
    "    interaction_data = interaction_data.drop(columns = ['customer_key', 'product_id']) \n",
    "    interaction_data['target'] = 1.0\n",
    "\n",
    "    # 9. Final Cleanup\n",
    "    interaction_data['item_price_std_items'] = interaction_data['item_price_std_items'].fillna(\n",
    "        interaction_data['item_price_std_items'].median()\n",
    "    )\n",
    "    \n",
    "    # CRITICAL: Calculate cat_dims on the FULL interaction_data BEFORE split\n",
    "    cat_dims = {col: interaction_data[col].nunique() for col in user_cat_cols + items_cat_cols}\n",
    "    user_cat_dims_map = {col : cat_dims[col] for col in user_cat_cols}\n",
    "    items_cat_dims_map = {col : cat_dims[col] for col in items_cat_cols}\n",
    "    \n",
    "    # Now split\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    df_train, df_test = train_test_split(interaction_data, test_size = 0.2, random_state = 142)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        TwoTowerDataset(\n",
    "            df_train, \n",
    "            user_num_cols=user_num_cols_final, \n",
    "            items_num_cols=item_num_cols_final, \n",
    "            user_cat_cols=user_cat_cols, \n",
    "            items_cat_cols=items_cat_cols\n",
    "        ), \n",
    "        batch_size=256, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        TwoTowerDataset(\n",
    "            df_test, \n",
    "            user_num_cols=user_num_cols_final, \n",
    "            items_num_cols=item_num_cols_final, \n",
    "            user_cat_cols=user_cat_cols, \n",
    "            items_cat_cols=items_cat_cols\n",
    "        ), \n",
    "        batch_size=256, \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Final training samples: {len(df_train):,}, Test samples: {len(df_test):,}\")\n",
    "    \n",
    "    return train_loader, test_loader, user_cat_dims_map, items_cat_dims_map, scaler, scaler_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1162b440-b056-4c18-bc02-dde630406e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 5. CLASS TWO TOWER DATASET\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset # Ensure Dataset is imported\n",
    "\n",
    "class TwoTowerDataset(Dataset):\n",
    "    \"\"\" Custom Dataset to handle user and item features separately. \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        df, \n",
    "        user_num_cols, \n",
    "        items_num_cols, \n",
    "        user_cat_cols, \n",
    "        items_cat_cols\n",
    "    ):\n",
    "        # 1. Store the column lists as instance attributes\n",
    "        self.user_num_cols = user_num_cols\n",
    "        self.items_num_cols = items_num_cols\n",
    "        self.user_cat_cols = user_cat_cols\n",
    "        self.items_cat_cols = items_cat_cols\n",
    "\n",
    "        # 2. Convert to PyTorch Tensors\n",
    "        self.user_num = torch.tensor(df[self.user_num_cols].values, dtype=torch.float32)\n",
    "        self.items_num = torch.tensor(df[self.items_num_cols].values, dtype=torch.float32)\n",
    "        self.user_cat = torch.tensor(df[self.user_cat_cols].values, dtype=torch.long)\n",
    "        self.items_cat = torch.tensor(df[self.items_cat_cols].values, dtype=torch.long)\n",
    "        self.target = torch.tensor(df['target'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            (self.user_num[idx], self.user_cat[idx]),\n",
    "            (self.items_num[idx], self.items_cat[idx]),\n",
    "            self.target[idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98527761-d6c0-48df-a4a9-f4f2dd6da36d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. CLASS TOWER\n",
    "# ==============================================================================\n",
    "\n",
    "class Tower(nn.Module):\n",
    "    \"\"\" A single Tower network, used for both User and Item feature vectors. \"\"\"\n",
    "    def __init__(self, num_dim, cat_dims_map, output_dim=embedding_dim):\n",
    "        super().__init__()\n",
    "        self.cat_embeddings = nn.ModuleList()\n",
    "        self.total_cat_emb_dim = 0\n",
    "        \n",
    "        for name, num_embeddings in cat_dims_map.items():\n",
    "            emb_dim = max(10, min(50, (num_embeddings // 2) + 1))\n",
    "            self.cat_embeddings.append(nn.Embedding(num_embeddings, emb_dim))\n",
    "            self.total_cat_emb_dim += emb_dim\n",
    "            \n",
    "        input_dim = num_dim + self.total_cat_emb_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(input_dim * 2, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, num_features, cat_features):\n",
    "        cat_emb_outputs = []\n",
    "        for i, embedding_layer in enumerate(self.cat_embeddings):\n",
    "            if cat_features.dim() == 1:\n",
    "                cat_input = cat_features\n",
    "            else:\n",
    "                cat_input = cat_features[:, i]\n",
    "            cat_emb_outputs.append(embedding_layer(cat_input))\n",
    "            \n",
    "        if cat_emb_outputs:\n",
    "            cat_features_combined = torch.cat(cat_emb_outputs, dim=1)\n",
    "            combined_features = torch.cat([num_features, cat_features_combined], dim=1)\n",
    "        else:\n",
    "            combined_features = num_features\n",
    "\n",
    "        embedding = self.mlp(combined_features)\n",
    "        return embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbc1a19-87a8-45bf-b56b-1ad6ded62a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. CLASS TWO TOWER MODEL\n",
    "# ==============================================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\" The main Two-Tower Model. \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        user_cat_dims_map, \n",
    "        items_cat_dims_map,\n",
    "        user_num_count,      \n",
    "        item_num_count,      \n",
    "        user_cat_cols_list,  \n",
    "        items_cat_cols_list  \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. User Tower Initialization\n",
    "        self.user_tower = Tower(\n",
    "            num_dim = user_num_count,\n",
    "            cat_dims_map = {k : user_cat_dims_map[k] for k in user_cat_cols_list} \n",
    "        )\n",
    "        \n",
    "        # 2. Items Tower Initialization\n",
    "        self.items_tower = Tower(\n",
    "            num_dim = item_num_count,\n",
    "            cat_dims_map = {k : items_cat_dims_map[k] for k in items_cat_cols_list}\n",
    "        )\n",
    "        \n",
    "        # Ensuring the output embedding dimensions match\n",
    "        assert self.user_tower.mlp[-1].out_features == self.items_tower.mlp[-1].out_features\n",
    "        \n",
    "    # The forward method\n",
    "    def forward(self, user_inputs, items_inputs):\n",
    "        user_num, user_cat = user_inputs\n",
    "        items_num, items_cat = items_inputs\n",
    "\n",
    "        user_embedding = self.user_tower(user_num, user_cat)\n",
    "        items_embedding = self.items_tower(items_num, items_cat)\n",
    "\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "        items_embedding = F.normalize(items_embedding, p=2, dim=1)\n",
    "\n",
    "        score_matrix = torch.matmul(user_embedding, items_embedding.T)\n",
    "        return score_matrix, user_embedding, items_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91d7701-5c19-497e-af07-29e9a021b455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. TRAIN TWO TOWER MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    \"\"\" Training loop with In-Batch Negative Sampling. \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"\\n--- Starting Training (In-Batch Negative Sampling) on {device} ---\")\n",
    "\n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for user_inputs, items_inputs, targets in train_loader:\n",
    "            user_num, user_cat = user_inputs\n",
    "            items_num, items_cat = items_inputs\n",
    "\n",
    "            user_inputs_device  = (user_num.to(device), user_cat.to(device))\n",
    "            items_inputs_device = (items_num.to(device), items_cat.to(device))\n",
    "\n",
    "            score_matrix, _, _ = model(user_inputs_device, items_inputs_device)\n",
    "            targets = torch.arange(score_matrix.shape[0]).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(score_matrix, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss (CrossEntropy): {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training Complete ---\")\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61389675-f613-4d4f-85d5-6f986310e9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 9. EVALUATE TWO TOWER MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader, k_values=[1, 5, 10]):\n",
    "    \"\"\" Evaluate the Two Tower model using standard recommendation metrics. \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    all_hits = {k: [] for k in k_values}\n",
    "    all_reciprocal_ranks = []\n",
    "    all_ndcg = {k: [] for k in k_values}\n",
    "    \n",
    "    # (Implementation detail: The remaining evaluation logic is unchanged)\n",
    "    with torch.no_grad():\n",
    "        for user_inputs, item_inputs, _ in test_loader:\n",
    "            user_num, user_cat = user_inputs\n",
    "            items_num, items_cat = item_inputs\n",
    "            \n",
    "            user_num = user_num.to(device)\n",
    "            user_cat = user_cat.to(device)\n",
    "            items_num = items_num.to(device)\n",
    "            items_cat = items_cat.to(device)\n",
    "            \n",
    "            user_emb = model.user_tower(user_num, user_cat)\n",
    "            items_emb = model.items_tower(items_num, items_cat)\n",
    "            \n",
    "            user_emb = F.normalize(user_emb, p=2, dim=1)\n",
    "            items_emb = F.normalize(items_emb, p=2, dim=1)\n",
    "            \n",
    "            score_matrix = torch.matmul(user_emb, items_emb.T)\n",
    "            \n",
    "            batch_size = score_matrix.shape[0]\n",
    "            _, indices = torch.sort(score_matrix, dim=1, descending=True)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                ranking = indices[i].cpu().numpy()\n",
    "                correct_item_rank = np.where(ranking == i)[0][0] + 1\n",
    "                \n",
    "                for k in k_values:\n",
    "                    hit = 1 if correct_item_rank <= k else 0\n",
    "                    all_hits[k].append(hit)\n",
    "                \n",
    "                all_reciprocal_ranks.append(1.0 / correct_item_rank)\n",
    "                \n",
    "                for k in k_values:\n",
    "                    if correct_item_rank <= k:\n",
    "                        dcg = 1.0 / np.log2(correct_item_rank + 1)\n",
    "                        idcg = 1.0 / np.log2(2)\n",
    "                        ndcg = dcg / idcg\n",
    "                    else:\n",
    "                        ndcg = 0.0\n",
    "                    all_ndcg[k].append(ndcg)\n",
    "    \n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        results[f'HR@{k}'] = np.mean(all_hits[k])\n",
    "        results[f'NDCG@{k}'] = np.mean(all_ndcg[k])\n",
    "    results['MRR'] = np.mean(all_reciprocal_ranks)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1634605d-579d-4381-9b5c-3090dde587b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nTRAINING WITH MULTIPLE RANDOM SEEDS\n======================================================================\n\n======================================================================\nSEED 1/5: 42\n======================================================================\n--- Starting Two-Tower Model Pipeline (Seed: 42) ---\nReading data from PostgreSQL table: retail_feature_engg_done...\nRead and converted 104,023 records to Pandas.\nFinal training samples: 79,783, Test samples: 19,946\n\n--- Starting Training (In-Batch Negative Sampling) on cpu ---\nEpoch 1/10, Loss (CrossEntropy): 5.0274\nEpoch 2/10, Loss (CrossEntropy): 4.8525\nEpoch 3/10, Loss (CrossEntropy): 4.8179\nEpoch 4/10, Loss (CrossEntropy): 4.7939\nEpoch 5/10, Loss (CrossEntropy): 4.7735\nEpoch 6/10, Loss (CrossEntropy): 4.7604\nEpoch 7/10, Loss (CrossEntropy): 4.7518\nEpoch 8/10, Loss (CrossEntropy): 4.7461\nEpoch 9/10, Loss (CrossEntropy): 4.7405\nEpoch 10/10, Loss (CrossEntropy): 4.7316\n--- Training Complete ---\n\n\uD83D\uDCCA Results for Seed 42:\n  HR@1:  0.3344\n  HR@10: 0.8190\n  MRR:   0.4960\n  ✅ NEW BEST MODEL! (HR@10: 0.8190)\n\uD83C\uDFC3 View run Two-Tower-Seed-42 at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192/runs/8b8dbd0827ea44c3a6072825e92001f2\n\uD83E\uDDEA View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192\n\n======================================================================\nSEED 2/5: 123\n======================================================================\n--- Starting Two-Tower Model Pipeline (Seed: 123) ---\n\n--- Starting Training (In-Batch Negative Sampling) on cpu ---\nEpoch 1/10, Loss (CrossEntropy): 5.0248\nEpoch 2/10, Loss (CrossEntropy): 4.8525\nEpoch 3/10, Loss (CrossEntropy): 4.8239\nEpoch 4/10, Loss (CrossEntropy): 4.7977\nEpoch 5/10, Loss (CrossEntropy): 4.7826\nEpoch 6/10, Loss (CrossEntropy): 4.7698\nEpoch 7/10, Loss (CrossEntropy): 4.7557\nEpoch 8/10, Loss (CrossEntropy): 4.7449\nEpoch 9/10, Loss (CrossEntropy): 4.7363\nEpoch 10/10, Loss (CrossEntropy): 4.7283\n--- Training Complete ---\n\n\uD83D\uDCCA Results for Seed 123:\n  HR@1:  0.3374\n  HR@10: 0.8186\n  MRR:   0.4992\n\uD83C\uDFC3 View run Two-Tower-Seed-123 at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192/runs/f04e5827232a4cfba0d0068476ceb6b9\n\uD83E\uDDEA View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192\n\n======================================================================\nSEED 3/5: 456\n======================================================================\n--- Starting Two-Tower Model Pipeline (Seed: 456) ---\n\n--- Starting Training (In-Batch Negative Sampling) on cpu ---\nEpoch 1/10, Loss (CrossEntropy): 5.0332\nEpoch 2/10, Loss (CrossEntropy): 4.8471\nEpoch 3/10, Loss (CrossEntropy): 4.8041\nEpoch 4/10, Loss (CrossEntropy): 4.7746\nEpoch 5/10, Loss (CrossEntropy): 4.7604\nEpoch 6/10, Loss (CrossEntropy): 4.7499\nEpoch 7/10, Loss (CrossEntropy): 4.7397\nEpoch 8/10, Loss (CrossEntropy): 4.7310\nEpoch 9/10, Loss (CrossEntropy): 4.7234\nEpoch 10/10, Loss (CrossEntropy): 4.7173\n--- Training Complete ---\n\n\uD83D\uDCCA Results for Seed 456:\n  HR@1:  0.3990\n  HR@10: 0.8610\n  MRR:   0.5586\n  ✅ NEW BEST MODEL! (HR@10: 0.8610)\n\uD83C\uDFC3 View run Two-Tower-Seed-456 at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192/runs/482f51875afc4b5cb3180aa7b61e17ff\n\uD83E\uDDEA View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192\n\n======================================================================\nSEED 4/5: 789\n======================================================================\n--- Starting Two-Tower Model Pipeline (Seed: 789) ---\n\n--- Starting Training (In-Batch Negative Sampling) on cpu ---\nEpoch 1/10, Loss (CrossEntropy): 5.0389\nEpoch 2/10, Loss (CrossEntropy): 4.8531\nEpoch 3/10, Loss (CrossEntropy): 4.8208\nEpoch 4/10, Loss (CrossEntropy): 4.7991\nEpoch 5/10, Loss (CrossEntropy): 4.7880\nEpoch 6/10, Loss (CrossEntropy): 4.7818\nEpoch 7/10, Loss (CrossEntropy): 4.7755\nEpoch 8/10, Loss (CrossEntropy): 4.7661\nEpoch 9/10, Loss (CrossEntropy): 4.7579\nEpoch 10/10, Loss (CrossEntropy): 4.7511\n--- Training Complete ---\n\n\uD83D\uDCCA Results for Seed 789:\n  HR@1:  0.2186\n  HR@10: 0.7471\n  MRR:   0.3834\n\uD83C\uDFC3 View run Two-Tower-Seed-789 at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192/runs/c05d8ab068a741bfb9b1190bf3110c7f\n\uD83E\uDDEA View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192\n\n======================================================================\nSEED 5/5: 2024\n======================================================================\n--- Starting Two-Tower Model Pipeline (Seed: 2024) ---\n\n--- Starting Training (In-Batch Negative Sampling) on cpu ---\nEpoch 1/10, Loss (CrossEntropy): 5.0330\nEpoch 2/10, Loss (CrossEntropy): 4.8502\nEpoch 3/10, Loss (CrossEntropy): 4.8093\nEpoch 4/10, Loss (CrossEntropy): 4.7774\nEpoch 5/10, Loss (CrossEntropy): 4.7620\nEpoch 6/10, Loss (CrossEntropy): 4.7528\nEpoch 7/10, Loss (CrossEntropy): 4.7440\nEpoch 8/10, Loss (CrossEntropy): 4.7349\nEpoch 9/10, Loss (CrossEntropy): 4.7248\nEpoch 10/10, Loss (CrossEntropy): 4.7184\n--- Training Complete ---\n\n\uD83D\uDCCA Results for Seed 2024:\n  HR@1:  0.3730\n  HR@10: 0.8496\n  MRR:   0.5338\n\uD83C\uDFC3 View run Two-Tower-Seed-2024 at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192/runs/3bf2dbae1bcd417391e798defc499fc4\n\uD83E\uDDEA View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192\n\n======================================================================\nMULTI-SEED TRAINING SUMMARY\n======================================================================\n\n\uD83D\uDCC8 Performance Statistics Across 5 Seeds:\n  HR@1:  0.3325 ± 0.0617\n  HR@10: 0.8191 ± 0.0397\n  MRR:   0.4942 ± 0.0600\n\n\uD83C\uDFC6 Best Model:\n  Seed:  456\n  HR@10: 0.8610\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/29 02:05:05 WARNING mlflow.models.model: Failed to validate serving input example {\n  \"inputs\": {\n    \"user_num\": [\n      [\n        -0.18218335509300232,\n        -0.21573451161384583,\n        -0.35019975900650024,\n        -0.3498169183731079,\n        -0.35586977005004883,\n        -0.3592550456523895,\n        -1.108251929283142,\n        -0.1604270190000534,\n        -0.14910228550434113,\n        -0.21633760631084442,\n        0.17039991915225983\n      ]\n    ],\n    \"user_cat\": [\n      [\n        49783\n      ]\n    ],\n    \"items_num\": [\n      [\n        -0.35160961747169495,\n        -0.3513142466545105,\n        -0.35710036754608154,\n        -0.3465733230113983,\n        -0.2786429226398468,\n        0.2788151204586029,\n        0.11060188710689545,\n        1.049331545829773,\n        1.055850863456726,\n        1.0033655166625977,\n        -1.2459255456924438\n      ]\n    ],\n    \"items_cat\": [\n      [\n        15687,\n        56,\n        2456\n      ]\n    ]\n  }\n}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\nGot error: No module named 'flask'\nRegistered model 'Retail_TwoTower_Recommender' already exists. Creating a new version of this model...\nCreated version '8' of model 'workspace_2327225092557172.default.retail_twotower_recommender'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Best model saved to MLflow (Run ID: ecf69201fee541be812a46a973bbb1b7)\n\uD83C\uDFC3 View run Two-Tower-BEST-Seed-456 at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192/runs/ecf69201fee541be812a46a973bbb1b7\n\uD83E\uDDEA View experiment at: https://dbc-9a4f847e-bb18.cloud.databricks.com/ml/experiments/1054156121569192\n\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 10. EXECUTION BLOCK WITH MULTI-SEED TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Try multiple seeds and keep the best model\n",
    "SEEDS_TO_TRY = [42, 123, 456, 789, 2024]\n",
    "best_hr10 = 0\n",
    "best_model_state = None\n",
    "best_seed = None\n",
    "all_results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING WITH MULTIPLE RANDOM SEEDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for seed_idx, seed in enumerate(SEEDS_TO_TRY):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"SEED {seed_idx + 1}/{len(SEEDS_TO_TRY)}: {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"Two-Tower-Seed-{seed}\") as run:\n",
    "        \n",
    "        print(f\"--- Starting Two-Tower Model Pipeline (Seed: {seed}) ---\")\n",
    "        \n",
    "        # 1. Read Data (only once, outside the seed loop would be better)\n",
    "        if seed_idx == 0:  # Read data only on first iteration\n",
    "            raw_df_pandas = read_and_convert_data(INPUT_TABLE_NAME, spark)\n",
    "            train_loader, test_loader, user_cat_dims_map, items_cat_dims_map, user_scaler, item_scaler = prepare_two_tower_data(raw_df_pandas)\n",
    "            \n",
    "            user_num_cols = train_loader.dataset.user_num_cols\n",
    "            items_num_cols = train_loader.dataset.items_num_cols\n",
    "            user_cat_cols = train_loader.dataset.user_cat_cols\n",
    "            items_cat_cols = train_loader.dataset.items_cat_cols\n",
    "            \n",
    "            user_num_count = len(user_num_cols)\n",
    "            items_num_count = len(items_num_cols)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params({\n",
    "            \"random_seed\": seed,\n",
    "            \"epochs\": 10,\n",
    "            \"batch_size\": 256,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"user_cat_count\": len(user_cat_dims_map),\n",
    "            \"item_cat_count\": len(items_cat_dims_map)\n",
    "        })\n",
    "        \n",
    "        # 2. Initialize Model (fresh initialization with new seed)\n",
    "        model = TwoTowerModel(\n",
    "            user_cat_dims_map = user_cat_dims_map,\n",
    "            items_cat_dims_map = items_cat_dims_map,\n",
    "            user_num_count     = user_num_count,      \n",
    "            item_num_count     = items_num_count,     \n",
    "            user_cat_cols_list = user_cat_cols,       \n",
    "            items_cat_cols_list= items_cat_cols       \n",
    "        )\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        # 3. Train\n",
    "        epoch_losses = train_model(model, train_loader, criterion, optimizer, epochs=10)\n",
    "        \n",
    "        # Log losses\n",
    "        for i, loss in enumerate(epoch_losses):\n",
    "            mlflow.log_metric(\"train_loss\", loss, step=i+1)\n",
    "        \n",
    "        # 4. Evaluate\n",
    "        model.eval()\n",
    "        metrics = evaluate_model(model, test_loader, k_values=[1, 5, 10, 20])\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'seed': seed,\n",
    "            'metrics': metrics,\n",
    "            'model_state': model.state_dict().copy()\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n\uD83D\uDCCA Results for Seed {seed}:\")\n",
    "        print(f\"  HR@1:  {metrics['HR@1']:.4f}\")\n",
    "        print(f\"  HR@10: {metrics['HR@10']:.4f}\")\n",
    "        print(f\"  MRR:   {metrics['MRR']:.4f}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if metrics['HR@10'] > best_hr10:\n",
    "            best_hr10 = metrics['HR@10']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_seed = seed\n",
    "            print(f\"  ✅ NEW BEST MODEL! (HR@10: {best_hr10:.4f})\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SUMMARY AND SAVE BEST MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MULTI-SEED TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate statistics\n",
    "hr1_scores = [r['metrics']['HR@1'] for r in all_results]\n",
    "hr10_scores = [r['metrics']['HR@10'] for r in all_results]\n",
    "mrr_scores = [r['metrics']['MRR'] for r in all_results]\n",
    "\n",
    "print(f\"\\n\uD83D\uDCC8 Performance Statistics Across {len(SEEDS_TO_TRY)} Seeds:\")\n",
    "print(f\"  HR@1:  {np.mean(hr1_scores):.4f} ± {np.std(hr1_scores):.4f}\")\n",
    "print(f\"  HR@10: {np.mean(hr10_scores):.4f} ± {np.std(hr10_scores):.4f}\")\n",
    "print(f\"  MRR:   {np.mean(mrr_scores):.4f} ± {np.std(mrr_scores):.4f}\")\n",
    "\n",
    "print(f\"\\n\uD83C\uDFC6 Best Model:\")\n",
    "print(f\"  Seed:  {best_seed}\")\n",
    "print(f\"  HR@10: {best_hr10:.4f}\")\n",
    "\n",
    "# Load best model and save it\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Save best model to MLflow\n",
    "with mlflow.start_run(run_name=f\"Two-Tower-BEST-Seed-{best_seed}\") as best_run:\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        \"random_seed\": best_seed,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 256,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"user_cat_count\": len(user_cat_dims_map),\n",
    "        \"item_cat_count\": len(items_cat_dims_map),\n",
    "        \"selection_method\": \"best_of_5_seeds\"\n",
    "    })\n",
    "    \n",
    "    # Log best model's metrics\n",
    "    best_metrics = [r['metrics'] for r in all_results if r['seed'] == best_seed][0]\n",
    "    mlflow.log_metrics(best_metrics)\n",
    "    \n",
    "    # Create signature for best model\n",
    "    try:\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        user_inputs, items_inputs, _ = sample_batch\n",
    "        user_num, user_cat = user_inputs\n",
    "        items_num, items_cat = items_inputs\n",
    "        \n",
    "        input_example_np = {\n",
    "            \"user_num\": user_num[:1].cpu().numpy(),\n",
    "            \"user_cat\": user_cat[:1].cpu().numpy(),\n",
    "            \"items_num\": items_num[:1].cpu().numpy(),\n",
    "            \"items_cat\": items_cat[:1].cpu().numpy(),\n",
    "        }\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        user_inputs_device = (user_num[:1].to(device), user_cat[:1].to(device))\n",
    "        items_inputs_device = (items_num[:1].to(device), items_cat[:1].to(device))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            score_matrix_out, user_emb_out, items_emb_out = model(user_inputs_device, items_inputs_device)\n",
    "        \n",
    "        input_schema = Schema([\n",
    "            ColSpec(\"double\", \"user_num\"),\n",
    "            ColSpec(\"long\", \"user_cat\"),\n",
    "            ColSpec(\"double\", \"items_num\"),\n",
    "            ColSpec(\"long\", \"items_cat\"),\n",
    "        ])\n",
    "        \n",
    "        output_schema = Schema([\n",
    "            ColSpec(\"double\", \"score_matrix\"),\n",
    "            ColSpec(\"double\", \"user_embedding\"),\n",
    "            ColSpec(\"double\", \"item_embedding\"),\n",
    "        ])\n",
    "        \n",
    "        model_signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to create signature: {e}\")\n",
    "        model_signature = None\n",
    "        input_example_np = None\n",
    "    \n",
    "    # Log the best model\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model=model, \n",
    "        artifact_path=\"two_tower_model\",\n",
    "        registered_model_name=\"Retail_TwoTower_Recommender\",\n",
    "        input_example=input_example_np,\n",
    "        signature=model_signature\n",
    "    )\n",
    "    \n",
    "    # Log scalers\n",
    "    with open(\"user_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(user_scaler, f)\n",
    "    with open(\"item_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(item_scaler, f)\n",
    "    \n",
    "    mlflow.log_artifact(\"user_scaler.pkl\", \"preprocessing\")\n",
    "    mlflow.log_artifact(\"item_scaler.pkl\", \"preprocessing\")\n",
    "    \n",
    "    print(f\"\\n✅ Best model saved to MLflow (Run ID: {best_run.info.run_uuid})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68571e33-e3fc-45b4-a281-6ebaf69bea64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ef8ff8-6112-484b-a7fb-8819e44b5aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542fc19b-8dc1-49f3-8404-86d1103eedaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f701a27e-9934-4cc4-9176-c7aea8bb2cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc48498-8cb0-4c60-9606-47228526e5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88dd181a-7cfc-4575-a35d-3e53d9b01fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b1291e-f25e-435d-98dc-abd05391cce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288c6593-1874-4733-8d52-5a5b3d9eeb9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771b02c9-2f04-4846-ac81-61608e0fabd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08dbcb20-5865-4984-8ebc-537e808a4639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7932e6-a4b6-4e53-81e4-de3abcac4c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f813fe8-fa69-47c2-bbb8-c6815b568b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92fa989-97b7-4135-8a08-072bab829f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed4ab66-55c1-4570-9c4a-497d01d226aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aa2de2-5034-4446-8f3b-26f24499e9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1aa9aaf-3e03-4dee-a8d2-263ae772c602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b887f3-b231-4df9-859b-94e4d67fa5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21fb4a1a-ee8c-41c9-8972-2b9299d46042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1680e09c-9abf-430e-a820-3c04b883b6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a60228-4f20-4746-8594-d521e0f27dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "575fb014-99af-425b-88fb-28b4211050b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e971c94-8083-4240-a6e7-68686bd28850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "463a24f4-f8c3-4def-a98f-b99e98555d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94419180-eb2c-4937-b5a1-1a4aba07dfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5986240-8fa6-49e1-bb87-f3ea4f43cae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RETAIL-PLATFORM-TWO-TOWER-MODEL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
